\documentclass[english]{cgspaper} % change option to 'english' to include english logo in \copyrightspace

%\usepackage[ngerman]{babel} % comment out to use english in auto-generated section titles
\usepackage[utf8]{inputenc}
\usepackage[ruled]{algorithm}
\usepackage{algpseudocode}
\usepackage{url}
\usepackage{csquotes}

\title{Demonstration of Game-Based Object Detection}
\author{Tom Beckmann\\ Digital Engineering Faculty, Hasso Plattner Institute \textbar{} University of Potsdam}
\author{Philipp Bode\\ Digital Engineering Faculty, Hasso Plattner Institute \textbar{} University of Potsdam}
\author{Julius C. R. Rudolph\\ Digital Engineering Faculty, Hasso Plattner Institute \textbar{} University of Potsdam}
\author{Hendrik Rätz\\ Digital Engineering Faculty, Hasso Plattner Institute \textbar{} University of Potsdam}

% Konfiguration des Veranstaltungs-Feldes
\subject{%
    \textbf{Advanced Games of Life}\\
    Sommersemester 2018\\
    Themenstellung und Anleitung:
    Daniel Limberger und Prof.\ Dr.\ Jürgen Döllner}

\begin{document}

% Definition des Teasers
%\teaser{
    %\includegraphics[width=0.9\textwidth]{graphics/prozess.pdf}
    %\caption{Beispiel für einen Teaser: Schritte beim Erstellen eines fachwissenschaftlichen Beitrags. Ein Teaser dient als Blickfang schon auf der ersten Seite eines Artikels.}
    %\label{fig:prozess}
%}

\maketitle

%----------------------------------------------------------------
% Zusammenfassung
%----------------------------------------------------------------
\begin{abstract}
\end{abstract}

\copyrightspace % Erzeugt den Hinweis auf die Veranstaltung links unten
\input{sections/introduction.tex}
%\input{context.tex}

%----------------------------------------------------------------
% Related Work
%----------------------------------------------------------------
\section{Related Work}
\label{sec:related_work}
Related projects/Work

% Wie analysieren wir diese?
\input{sections/concept.tex}

%----------------------------------------------------------------
% Implementation
%----------------------------------------------------------------

% Implementierung des Spielkonzepts (vermutlich mit Fokus auf AR)
% Limitierungen von AR (bzw. Wie gehen wir damit um?)

\section{Gameplay Implementation}
\label{sec:gameplay_implementation}
% - game uses AR to capture demons; involves getting idea about the location, correlating it to 2D frames that are analyzed by pipeline
In this section, we will outline the implementation of Demon GO.
We will first describe how the advanced gameplay elements for player vs player interactions are handled, then demonstrate how the augmented reality component allows players to capture demons, and then move on to the data exploitation subsystem.
Snapshots from the augmented reality component form the bridge between the game and the exploitation subsystems.
The exploitation subsystem consists of a light weight image processing pipeline on the player's phone and a server component for heavier processing.

\subsection{Map View}
- using MapBox to display an interactive map of the augmented world with markers at the positions of stashes (different colors for own and hostile stashes) with their perimeter/range of influence
- for own stashes the user also sees icons for every demon he placed to defend the stash 

- markers are pinned by geo-location on the map

\subsection{Storing and syncing stash data}
- need to easily sync the current game state (players, stashes, demons) across multiple clients on different devices

- using document-based NoSQL-database Google Firestore (cloud hosting, synced state across clients, easy setup) --> current flagship database of Google for mobile app development

- document hierarchy: player id --> stashes/null stash --> demons for stash (everything indexed by id)

- App implements change listeners for the collections of stashes and demons --> depending on what type of event happened stash/demon markers on the map (ideally only nearby) need to be redrawn
--> updated after every fight and every placement of a demon onto a stash

- demons that the player didn't place at a stash are saved in the player's "null-stash" which is not saved in the collection of stashes (as it does not need to be synced with the other players)
- updated when new demon was summoned or captured and after every fight in which the attacking demon lost health points

\subsection{Demon fights}

- the current implementation is a simulation that follows a round-based approach. In every round the attacking demon assaults one defending demon and afterwards gets assaulted by every defending demon sequentially. Thereby both the order in which the defenders are attacked by the attacker and the order in which the defenders counter-attack the attacking demon are shuffled once in advance of the fight. 

- makes the fight more unpredictable and harder for the attacker to guess which demons he might eliminate. After a fight both users are informed about the result (using Android Toast/Push Notification) and the health points of the surviving demons are updated in the corresponding Google Firestore documents.

- as users can currently add an unlimited amount of demons to a stash the defense of a stash gets exponentially stronger the more demons the defender places on them

- in a future version: defender can interactively intervene in the fight and dynamically add more defenders to the stash -> therefore fight must obviously take a while

\input{sections/implementation-arcore.tex}

\section{Data Analysis Implementation}
\begin{figure*}
    \includegraphics[width=\textwidth]{graphics/pipeline_phase_1.png}
    \caption{Pipeline Scanning Phase}
    \label{fig:prozess}
\end{figure*}
\begin{figure*}
    % \includegraphics[width=\textwidth]{graphics/pipeline_phase_1.png}
    \caption{Pipeline Capturing(Detail) Phase}
    \label{fig:prozess}
\end{figure*}

% FIXME intro to OpenCV, features, usages?
- pipeline which processes every frame (captured by camera while capturing demons) on phone
- preselect frames which are sent to server
- only best should get sent
- pipeline consists of independent steps which could be ordered in any order -> same interface
- frames are wrapped in \textit{Snapshots} (ensures that the same object is passed between steps)

\subsection{Snapshots / Communication with Pipeline}
% FIXME in parts dupl with ARCore.Snapshots above -- might be fine if we focus on the pipeline-specific aspects
- holds OpenCV Mat (represents frame) and score (calculated in steps)
- also able to convert Mat to base64 and make a parameter list out of attributes (both for sending) 

- AR-Snapshots: additional fields (points in room and viewProjectionMatrix)

\subsection{Steps}
- contain list of following steps (in next)
- able to track time of execution
- .start(snap) -> .process(snap) -> .output(snap) (starts all steps in next with processed snapshots)

- special case: \textit{StepWithQueue}
- additional priority queue based on snapshot score
- getBestAndClear (angleChange) and getBest (sendingStep)
- used in angleChange and sendingStep where the only the best frame should be handled/should be handled first

% Daten-Analyse-Pipeline
 % Finden von PoI
 % Capturing und Processing
 
 \subsubsection{Blur Estimation}
 The Blur Estimation is needed to preselect snapshots whose quality is sufficient for further processing. 
 The frame of the snapshot, which is processed, will be converted to a grayscale image before the Laplace operator is applied to it.
 Afterwards the variance of the new image will be calculated and will indicate the blurriness where a higher variance will represent a sharper image.
 The idea behind this approach is that a blurry image (because of its nature) is less likely to have a lot of clear edges. 
 Therefore the variance of the Laplace operator, which is used to detect edges, will be lower than in a sharp picture. 
 
 \subsubsection{AngleChange}
 
\subsubsection{Brand/Pattern Recognition}
- Utilizes keypoint detection and feature descriptor matching to match recognizable patterns like brand logos and letterings \\
- The ORB (Oriented FAST and Rotated BRIEF) technique is used \cite{rublee2011orb} \\
- Computationally expensive step is keypoint calculation for frame.\\
-> Comparison with large number of patterns possible, as their keypoints can be precalculated.\\
- 'cheaper' than neural network, especially on mobile devices\\
- much better results than template matching

\subsubsection{Contour Detection}
- Gaussian Blur -> bilateral filter -> Canny edge detection -> find Contours and cull by minimum size and maximal edges.\\
- Found contours are cropped and passed to next steps for likelihood of text estimation

\subsubsection{Noise Estimation}
- A simple kernel for fast noise estimation for which the authors note \enquote{In textured images or regions, though, the noise estimator perceives thin lines as noise.} \cite{immerkaer1996fast}. \\
- We utilize this property to detect probable regions with dense text.

\subsubsection{Colorfulness Estimation}
- Simple measure for estimating colorfulness of an image \cite{hasler2003measuring}. \\
- We presume that regions of legible text are usually low in color variance to increase the contrast between letters and background.

\subsection{Communication between phone and server}

After a snapshot was processed in the pipeline it is added to the priority queue of the sending step where all snapshots are ordered by their calculated score.
The transmission itself is handled by \textit{Volley} a fast Android networking library.
Every 0.5s a POST-request will be prepared including the base64 encoded frame of the best snapshot in the priority queue and the id of the player. 
After being added to the request queue of Volley, the request will then be sent to the server as soon as possible.

Additionally every 100th frame, which enters the pipeline will be added to the queue of the sending step directly after passing the blur estimation and angle change step.
This ensures that once in a while a data sent to the server if no other snapshot has a sufficient score after passing the other steps.
The score generation, however, guarantees that this very rarely the case.

% Direct sending after brand detection

\subsection{Server-side Analysis}

- Flask/Flask-SocketIO server with sqllite db
- decodes received images and writes them to the disk (for now)
- detects where possible text is written on saved image
- crops image for every box \& saves it
- analyses rotaion of text, removes rotation and runs pytesseract on it 
- saves user\_id, filename of cropped img, analysed text, rotation, timestamp and confidence of analysis (given by pytesseract) to db
- also informs the frontend (?) about added and processed images via SocketIO


%----------------------------------------------------------------
% Evaluation
%----------------------------------------------------------------

\input{sections/evaluation.tex}

%----------------------------------------------------------------
% Conclusion
%----------------------------------------------------------------
\section{Conclusion}
\label{sec:conclusion}

% Future Work
- Move away from contour-based region of interest recognition and aggregate 
- z.B. weitere Daten sammeln und komplexere Nutzerprofile anlegen
- Data Analytics auf den gesammelten Infos

%----------------------------------------------------------------
% Sources
%----------------------------------------------------------------
\bibliographystyle{acmsiggraph}
\bibliography{foo-paper}

\end{document}
