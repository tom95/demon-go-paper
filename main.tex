\documentclass[english]{cgspaper} % change option to 'english' to include english logo in \copyrightspace

%\usepackage[ngerman]{babel} % comment out to use english in auto-generated section titles
\usepackage[utf8]{inputenc}
\usepackage[ruled]{algorithm}
\usepackage{algpseudocode}
\usepackage[hyphens]{url}
\usepackage{csquotes}
\usepackage{subcaption}
\usepackage{hyperref}

%\usepackage[backend=biber,style=authoryear, citestyle=authoryear]{biblatex}
%\addbibresource{foo-paper.bib}

\title{Demonstration of Game-Based Object Detection}
\author{
    Tom Beckmann, Philipp Bode,\\ Julius C. R. Rudolph, Hendrik Rätz\\ Digital Engineering Faculty, Hasso Plattner Institute \textbar{} University of Potsdam
}
% \author{}
% \author{Julius C. R. Rudolph\\ Digital Engineering Faculty, Hasso Plattner Institute \textbar{} University of Potsdam}
% \author{Hendrik Rätz\\ Digital Engineering Faculty, Hasso Plattner Institute \textbar{} University of Potsdam}

% Konfiguration des Veranstaltungs-Feldes
\subject{%
    \textbf{Advanced Games of Life}\\
    Sommersemester 2018\\
    Themenstellung und Anleitung:
    Daniel Limberger und Prof.\ Dr.\ Jürgen Döllner}

\begin{document}

% Definition des Teasers
%\teaser{
    %\includegraphics[width=0.9\textwidth]{graphics/prozess.pdf}
    %\caption{Beispiel für einen Teaser: Schritte beim Erstellen eines fachwissenschaftlichen Beitrags. Ein Teaser dient als Blickfang schon auf der ersten Seite eines Artikels.}
    %\label{fig:prozess}
%}

\maketitle


%----------------------------------------------------------------
% Zusammenfassung
%----------------------------------------------------------------
\begin{abstract}
With the emergence of easy-to-use Augmented Reality (AR) frameworks and increasing capabilities of modern smartphones, the potential for automated analysis of the user's camera feed arises.
However, malicious actors could employ such analyses for illicit surveillance or unethical profiling use cases, all the while staying hidden from the end user.
We propose a showcase application that demonstrates two of these use cases. In the context of an AR game, we scan the user's perimeter for brand logos and try to lead the player into providing close-up shots of potentially sensitive textual information.
Even though the app is prototypical, through it one can foresee the severe potential for misuse of soon-to-be widespread AR applications.
\end{abstract}

\copyrightspace % Erzeugt den Hinweis auf die Veranstaltung links unten
\input{sections/introduction.tex}
%\input{context.tex}


% Wie analysieren wir diese?
\input{sections/concept.tex}

%----------------------------------------------------------------
% Implementation
%----------------------------------------------------------------

% Implementierung des Spielkonzepts (vermutlich mit Fokus auf AR)
% Limitierungen von AR (bzw. Wie gehen wir damit um?)

\section{Gameplay Implementation}
\label{sec:gameplay_implementation}
% - game uses AR to capture demons; involves getting idea about the location, correlating it to 2D frames that are analyzed by pipeline
In this section, we will outline the implementation of Demon GO.
We will first describe how the advanced gameplay elements for player vs player interactions are handled, then demonstrate how the augmented reality component allows players to capture demons, and then move on to the data exploitation subsystem.
Snapshots from the augmented reality component form the bridge between the game and the exploitation subsystems.
The exploitation subsystem consists of a lightweight image processing pipeline on the player's phone and a server component for heavier processing.

\subsection{Handling and Displaying the Game State}

Demon GO is using the live location platform \cite{Mapbox} Mapbox to display an interactive map of the augmented world with markers at the geo-locations of stashes with their range of influence.
For own stashes, the user also sees icons representing every demon he placed to defend the stash implemented as different map layers that each contains the corresponding demon collection placed on a stash.

%\subsection{Storing and Syncing Stash Data}
To allow a multiplayer live interaction it was necessary to easily sync the current game state (players, stashes, demons) across multiple clients on different devices. 
Therefore we decided to use the document-based NoSQL-database Google Firestore, Google's current flagship database for mobile app development, which offers easy setup, cloud hosting and uncomplicated state syncing across clients.

%The database consists of three document collections that  documents are saved in the following hierarchy: for every player id --> stashes/null stash --> demons for stash (everything indexed by id)

The app implements change listeners for the collections of all stashes and their defending demons and depending on what type of change event happened either only the updated stashes or also their defending demon markers on the map are redrawn.
Both collections get updated after every fight and every placement of a demon onto a stash.
Furthermore, a stash gets updated when the user deposits EP into or withdraws EP from it.
Whenever the radius of a stash changes (as a consequence of a change in the aggregated defenders' HP) the distance to every other stash minus its radius is computed to gain the maximal possible stash radius.
As we currently only have few stashes we listen for changes on every stash. 
For an upcoming version, we could filter the change listeners on nearby stashes using built-in Firestore query selectors.

Demons that the player didn't place at a stash are saved in the player's \emph{null-stash} which is saved in the player's collection instead of the collection of stashes as it does not need to be synced with the other players.
The \emph{null-stash} is only updated when a new demon was summoned or captured and after every fight in which the attacking demon lost HP.

\subsection{Demon Fights}

Demon fights are currently implemented as a simulation that follows a round-based approach. In every round the attacking demon assaults one defending demon and afterwards gets assaulted by every defending demon sequentially. 
Therefore both the order in which the defenders are attacked by the attacker and the order in which the defenders counter-attack the attacking demon are shuffled once in advance of the fight. 
This makes the fight more unpredictable and harder for the attacker to guess which demons he might eliminate. After a fight both users are informed about the result and the HP of the surviving demons are updated in the corresponding Google Firestore documents.

As users can currently add an unlimited amount of demons to a stash the defense of a stash gets exponentially stronger the more demons the defender places on them.
In a later version it would probably be sensible to introduce limits to the maximal amount of defending demons.

\input{sections/implementation-arcore.tex}

\section{Data Analysis Implementation}

The major part of the data analysis is done in the scanning phase by the pipeline which can be seen in \autoref{fig:pipeline_phase1}.
The pipeline is started as soon as the AR subsystem hands over as snapshot.
In addition to the features described in \autoref{sec:snapshots} a score is added to the snapshot to determine its quality.
Furthermore, an offset will be tracked if the original frame will be cropped and only a small part is used for further processing. \\
Every snapshot will then be analyzed with the intent to minimize the traffic between the client and the server by sending only the best frames.
The server-side analysis will then yield points of interest based on those frames.

\begin{figure}[ht]
    \centering
    \includegraphics[width=8cm]{graphics/blur_grid_sep.png}
    \caption{Sharp images (with and without Laplace operator) on the left and analogous blurry images on the right}
    \label{fig:blur_estimation}
\end{figure}

\begin{figure*}[t]
  \centering
  \subcaptionbox{Original  input frame.\label{fig:cont_dect1}}[.3\linewidth][c]{%
    \includegraphics[height=7.2cm]{graphics/contours/original.jpg}}\quad
  \subcaptionbox{Preprocessed and binarized  frame.\label{fig:cont_dect2}}[.3\linewidth][c]{%
    \includegraphics[height=7.2cm]{graphics/contours/canny.jpg}}\quad
  \subcaptionbox{Selected contours, passing contours are marked in green.\label{fig:cont_dect3}}[.3\linewidth][c]{%
    \includegraphics[height=7.2cm]{graphics/contours/finished.jpg}}

  \caption{Detection of contours in a real-word scene.}\label{fig:cont_dect}
\end{figure*}

\subsection{Steps}

The \emph{step} object is the abstract representation of each possible step of the pipeline and therefore each implementation provides the same interface to ensure interchangeability and reusability.
After a step is executed via calling the \emph{start} function the \emph{process} method is called and will, as the name implies, do the actual processing of a frame.
Afterward, the \emph{output} method will start all its following steps with the possibly new snapshot.\\
A special case of a step is the \emph{StepWithQueue} class where each snapshot is put into a priority queue based on its score.
This is useful in cases where only the best frame should be processed or where it should be processed first.
In this section we will explain the specific steps used in the scanning phase pipeline shown in \autoref{fig:pipeline_phase1} in detail.
% Daten-Analyse-Pipeline
 % Finden von PoI
 % Capturing und Processing
 
 
\textbf{Blur Estimation: } 
The first step of the pipeline is needed to preselect snapshots of which the quality is sufficient for further processing. 
The frame of the snapshot, which is processed, will be converted to a grayscale image before the Laplace operator is applied to it.
Examples of possible results after applying the Laplace operator can be seen in \autoref{fig:blur_estimation}.
Afterward, the variance of the new image will be calculated and will indicate the blurriness where a higher variance will represent a sharper image.
The idea behind this approach is that a blurry image (because of its nature) is less likely to have a lot of clear edges. 
Therefore the variance of the Laplace operator, which is used to detect edges, will be lower than in a sharp picture. 
For instance, the variance of the sharp picture displayed in \autoref{fig:blur_estimation} has a variance of 1152, whereas the variance of the blurry one is only 7. 
 
 \textbf{Angle Change: }
Subclassing from \emph{StepWithQueue}, the Angle Change Step won't process each snapshot individually but rather place them in a priority queue.
Only when the phone has been moved more than 10 centimetres in any direction or has been rotated by more than 5 degrees the best snapshot will be passed to the next steps.
Eventually, the queue will be cleared of all the remaining snapshots.
 
\textbf{Pattern Recognition: }
One of our assumed use cases for the game-based object detection is the recognition of known patterns, for example brand logos or in more extreme cases symbols that show dissident tendencies.
This is a classic use case for a feature matching approach.
We utilize an ORB (Oriented FAST and Rotated BRIEF) key point descriptor described by Rublee et al. that is rotation invariant and robust to noise, and therefore well applicable to real-world scenes~\cite{rublee2011orb}.
In the initialization of this step, the descriptors for all input logos are calculated and stored for later comparison.
When a frame enters the step, key points are calculated and compared to the known logos with a brute force matcher using a hamming distance as the measurement.
Every pairing falling under a fixed threshold is considered a match, and the mobile client sends a sighting confirmation to the data backend.
We were able to choose this threshold in a way that false positives were nonexistent in our tests, while still providing a satisfying match rate.
As the descriptor calculation for the frame takes around a hundred times longer than the actual matching with a comparison image, we can search for a large number of logos before performance concerns become relevant.
The brand matching use case, therefore, does not rely on computationally expensive machine learning models.


% \begin{figure}%
%     \centering
%     \subfloat[Frame input\label{fig:cont_dect1}]{{\includegraphics[height=7.2cm]{graphics/contours/original.jpg}}}%
%     \qquad
%     \subfloat[Preprocessed frame\label{fig:cont_dect2}]{{\includegraphics[height=7.2cm]{graphics/contours/canny.jpg}}}%
%     \qquad
%     \subfloat[Selected contours\label{fig:cont_dect3}]{{\includegraphics[height=7.2cm]{graphics/contours/finished.jpg}}}%
%     \caption{Detection of contours in a real-word scene.}%
%     \label{fig:cont_dect}
% \end{figure}
\textbf{Contour Detection: }
As a starting point to check areas of the incoming frame for potentially sensitive information, we detect contours in the scene.
We reason that this information can most likely be found in areas discernible from the rest of the frame, for example, a piece of paper.
The frame is preprocessed in several steps to maximize recognizable contours:
First, we apply a Gaussian Blur with a small kernel to reduce image noise, and then apply a bilateral filter to smooth the image further while keeping edges intact.
Afterward, a Canny edge filter with a dynamic threshold is applied to binarize the frame.
The result of this preprocessing can be found in \autoref{fig:cont_dect1}.
On this image, we run a contour detection algorithm provided by OpenCV to retrieve a list of contours.
Contours with a small size beneath a fixed threshold are culled from this list to further sort out \enquote{scene noise} like the numerous objects on the left side of the desk.
The outlines of these contours are approximated by polygons for which the deviation epsilon is based on the contour's area.
Once again we sort out polygons with more than seven edges to reduce our set of candidates.
All contours with a large enough area can be seen in \autoref{fig:cont_dect3}. Those with simple enough approximations are colored green.
We can see that from the complex original scene, only three areas of the image remain.
However, one potentially interesting area in the foreground is left out and thus will not be considered in later steps of the pipeline.
Each selected contour is cropped from the image and will be passed forward to the next step.

\textbf{Noise Estimation: }
To determine which of the found contours are likely to contain (textual) information, we first calculate a noise measure.
We use a simple kernel for fast noise variance estimation proposed by Immerkaer.~\cite{immerkaer1996fast}.
The author notes that \enquote{In highly textured images or regions, though,
the noise estimator perceives thin lines as noise}.
We utilize this property of the estimator as a criterium for the detection of small, dense text.
Areas with a low noise value are discarded in this step.
It is, however, completely plausible that high-noise contours do not contain text.
The score of the snapshot is multiplied with the noise value, and if the noise threshold is passed, the snapshot is passed on to the next step.

\textbf{Colorfulness Estimation: }
To further preselect the remaining contours, we estimate the colorfulness of the image regions.
A fast method for this is proposed by Hasler, meant to be used as an overall colorfulness estimation of a natural scene.~\cite{hasler2003measuring}.
We presume that regions of legible text are usually low in color variance to increase the contrast between letters and background.
Regions with high color variance are therefore not passed forward to the next step.
The score is once again multiplied with the colorfulness value.
In practice, contours with a high amount of noise and a low color variance are good estimates for regions of the image that actually contain text.
The scores of the two estimation measures on the found contours and a piece of wall and desk for comparison's sake can be seen in \autoref{fig:estimators}.
We can see that the letter does indeed possess a high noise score, but the colorfulness is not especially different from the other low-color contours.
We therefore send each contour passing the preselect steps to the data backend for further analysis of scene text occurrences.

\begin{figure}[t]
\centering
  \subcaptionbox{\textbf{N:} 0.9, \textbf{C:} 11.9\label{fig:estimators1}}{%
    \includegraphics[height=3cm]{graphics/estimators/cover1.jpg}}\quad
  \subcaptionbox{\textbf{N:} 0.95, \textbf{C:} 15.5\label{fig:estimators2}}{%
    \includegraphics[height=3cm]{graphics/estimators/cover2.jpg}}\quad
  \subcaptionbox{\textbf{N:} 1.29, \textbf{C:} 13.1\label{fig:estimators3}}{%
    \includegraphics[height=3cm]{graphics/estimators/letter.jpg}}
\newline
\centering
  \subcaptionbox{\textbf{N:} 0.6, \textbf{C:} 1.8\label{fig:estimators4}}{%
    \includegraphics[height=2.5cm]{graphics/estimators/wall.png}}
  \subcaptionbox{\textbf{N:} 0.8, \textbf{C:} 30.3\label{fig:estimators5}}{%
    \includegraphics[height=2.5cm]{graphics/estimators/desk.png}}

  \caption{Noise (N) and Colorfulness (C) estimators applied to dewarped contours and wall and desk textures for comparison.}\label{fig:estimators}
\end{figure}

\subsection{Communication Between Phone and Server}

After a snapshot was processed in the pipeline it is added to the priority queue of the sending step where all snapshots are ordered by their calculated score.
The transmission itself is handled by \emph{Volley} a fast Android networking library~\cite{volley}.
Every 0.5s a POST-request will be prepared including the base64 encoded frame of the best snapshot in the priority queue and the id of the player. 
After being added to the request queue of Volley, the request will then be sent to the \emph{detect\_text} endpoint of the server as soon as possible.

Additionally, every 100th frame, which enters the pipeline will be added to the queue of the sending step directly after passing the blur estimation and angle change step.
This ensures that once in a while a data sent to the server if no other snapshot has a sufficient score after passing the other steps.
The score generation, however, guarantees that this very rarely the case.

In two special cases, requests are directly queued into Volley's request queue and avoid being processed in the sending step.
This is the case if a brand is successfully detected in the brand detection step or when the user successfully casts a spell in the demon capturing phase. 
In the first case, only the user id and the detected brand are transmitted as string whereas in the second one the full camera frame is transmitted alongside the user id.

\subsection{Server-side Analysis}

The backend is based on the Python framework \emph{Flask}~\cite{flask} and provides multiples endpoints where it accepts data sent from the Android Clients.

Frames that were processed in the scanning pipeline will be sent to the \emph{detect\_text} endpoint where a text detection will be applied.
A frame which passes the pipeline is likely to include text but to guarantee that this is the case and not only random noise was found, a text detection is applied. 
The \emph{EAST scene text detection} proposed by Zhou et al. determines quadrilateral shapes around possible text areas by feeding the supplied frame into a pre-trained neural network~\cite{Zhou2017EASTAE}.
If text is found, the server calculates the center of all found shapes and sends a response with the coordinates back to the client, where the snapshot can project these values into a three-dimensional real-world point of interest.

If a close-up of a point of interest is taken in the second game phase, it will be transmitted via the \emph{ocr} route. 
This will trigger a more precise analysis of the received frame involving the \emph{EAST scene text detection} and \emph{pytesseract}~\cite{pytesseract}, which is a Python wrapper for Google's OCR framework \emph{Tesseract}~\cite{tesseract}.
The text detection is applied to the frame and the resulting text boxes will be processed individually in the following steps.
First, the image is rotated to ensure that the text is aligned with the borders of the image.
Subsequently, the image is converted to grayscale as well as eroded and dilated in order to remove potential noise.
A bilateral filter is then applied to receive an image which is only black and white because an OCR algorithm will work best under those circumstances.
Finally, said OCR algorithm which is provided by \emph{pytesseract} will be applied to the image.
Because of the preprocessing, it tries to interpret the given image as a single line of text which is either English or German. 
The resulting string will be saved to an SQLite database \cite{sqlite} together with the user id of the Android client, the filename of the cropped text box and a timestamp.

The third and last endpoint is called \emph{brand} and is used to store the user id, timestamp and the name of a recognized brand in the database.

% Frontend?

%----------------------------------------------------------------
% Evaluation
%----------------------------------------------------------------

\input{sections/evaluation.tex}

%----------------------------------------------------------------
% Conclusion
%----------------------------------------------------------------
\input{sections/conclusion.tex}
%----------------------------------------------------------------
% Sources
%----------------------------------------------------------------
\bibliographystyle{acmsiggraph}
\bibliography{foo-paper}
%\printbibliography 

\end{document}
